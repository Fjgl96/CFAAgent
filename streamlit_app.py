# streamlit_app.py
"""
Aplicaci√≥n Streamlit - Agente Financiero con RAG.
Actualizado para LangChain 1.0+ con LangSmith.
"""

import streamlit as st
from langchain_core.messages import HumanMessage, AIMessage
import uuid

# Importar el grafo compilado
try:
    from graph.agent_graph import compiled_graph
    print("‚úÖ Grafo importado correctamente en streamlit_app.")
except Exception as import_error:
    st.error(f"Error cr√≠tico al importar el agente: {import_error}")
    print(f"‚ùå Error cr√≠tico importando 'compiled_graph': {import_error}")
    st.stop()

# Importar config para mostrar info de LangSmith
from config import LANGSMITH_ENABLED
import os

# --- Configuraci√≥n de P√°gina Streamlit ---
st.set_page_config(
    page_title="Agente Financiero Pro",
    layout="centered",
    initial_sidebar_state="auto"
)

# --- T√≠tulo y Cabecera ---
st.title("üí∞ Agente Financiero Profesional")
st.caption("Impulsado por LangGraph, Anthropic Claude y RAG (Elasticsearch)")

# Mostrar info de LangSmith si est√° habilitado
if LANGSMITH_ENABLED:
    st.info(f"üîç **LangSmith activo** - Proyecto: `{os.environ.get('LANGCHAIN_PROJECT', 'N/A')}`")

st.markdown("""
Esta es una calculadora financiera inteligente con acceso a documentaci√≥n CFA. Puedes:

**üìä Realizar c√°lculos:**
- Valor Actual Neto (VAN)
- Costo Promedio Ponderado de Capital (WACC)
- Valoraci√≥n de Bonos
- CAPM, Sharpe Ratio, Gordon Growth, Opciones Call

**üìö Consultar documentaci√≥n CFA:**
- "¬øQu√© dice el CFA sobre el WACC?"
- "Explica el concepto de Duration"
- "Busca informaci√≥n sobre el modelo Gordon Growth"

**‚ùì Obtener ayuda:**
- "Ayuda" o "¬øQu√© puedes hacer?"
""")
st.divider()

# --- L√≥gica del Chat ---

# Inicializar historial de chat si no existe
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "¬°Hola! ¬øQu√© c√°lculo financiero necesitas realizar hoy? Tambi√©n puedo consultar la documentaci√≥n CFA si tienes preguntas te√≥ricas."}
    ]

# Inicializar thread_id √∫nico por sesi√≥n si no existe
if "thread_id" not in st.session_state:
    st.session_state.thread_id = str(uuid.uuid4())
    print(f"Nuevo Thread ID generado para la sesi√≥n: {st.session_state.thread_id}")

# Mostrar mensajes del historial
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Capturar input del usuario
if prompt := st.chat_input("Ej: Calcula VAN: inversi√≥n 50k, flujos [15k, 20k, 25k], tasa 12%"):
    
    # A√±adir mensaje del usuario al historial y mostrarlo
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Preparar entrada para LangGraph
    graph_input = {"messages": [HumanMessage(content=prompt)]}
    
    # Usar el ID de sesi√≥n √∫nico guardado en st.session_state
    config = {"configurable": {"thread_id": st.session_state.thread_id}}
    
    # Ejecutar el grafo y mostrar respuesta del asistente
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        with st.spinner("Calculando... üß†"):
            final_response_content = ""
            
            try:
                # Usar invoke() para obtener solo el estado final
                final_state = compiled_graph.invoke(graph_input, config=config)
                
                # Extraer la respuesta final
                if final_state and "messages" in final_state and final_state["messages"]:
                    for msg in reversed(final_state["messages"]):
                        is_final_ai_msg = isinstance(msg, AIMessage) and not getattr(msg, 'tool_calls', [])
                        if is_final_ai_msg:
                            content = msg.content
                            if isinstance(content, str):
                                final_response_content = content
                            elif isinstance(content, list):
                                text_parts = []
                                for part in content:
                                    if isinstance(part, dict) and 'text' in part:
                                        text_parts.append(part['text'])
                                    elif isinstance(part, str):
                                        text_parts.append(part)
                                final_response_content = "\n".join(text_parts).strip()
                            
                            if final_response_content:
                                break
                
                if not final_response_content:
                    final_response_content = "Lo siento, no pude procesar tu solicitud completamente. ¬øPodr√≠as intentarlo de nuevo o reformular?"
                    print("‚ö†Ô∏è No se encontr√≥ AIMessage final √∫til en el estado final.")
            
            except Exception as e:
                final_response_content = f"Ocurri√≥ un error inesperado al procesar tu solicitud."
                import traceback
                error_details = traceback.format_exc()
                print(f"‚ùå ERROR STREAMLIT RUNTIME: {error_details}")
                st.error(f"{final_response_content} Por favor, intenta de nuevo m√°s tarde.")
            
            # Mostrar la respuesta final
            if final_response_content:
                message_placeholder.markdown(final_response_content)
            else:
                message_placeholder.error("No se pudo obtener una respuesta.")
    
    # A√±adir respuesta final (o error) al historial de Streamlit
    if final_response_content:
        st.session_state.messages.append({"role": "assistant", "content": final_response_content})