# streamlit_app.py
import streamlit as st
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
import time # Para simular "escritura"

# Importar el grafo compilado desde graph.agent_graph
# El manejo de errores ahora est√° dentro de agent_graph.py
try:
    from graph.agent_graph import compiled_graph
    print("‚úÖ Grafo importado correctamente en streamlit_app.")
except Exception as import_error:
     # Si la importaci√≥n falla (ej. error de sintaxis en agent_graph), mostrar y detener
     st.error(f"Error cr√≠tico al importar el agente: {import_error}")
     print(f"‚ùå Error cr√≠tico importando 'compiled_graph': {import_error}")
     st.stop()


# --- Configuraci√≥n de P√°gina Streamlit ---
st.set_page_config(
    page_title="Agente Financiero Pro", 
    layout="centered", 
    initial_sidebar_state="auto" 
)

# --- T√≠tulo y Cabecera ---
st.title("üí∞ Agente Financiero Profesional")
st.caption("Impulsado por LangGraph y Anthropic Claude-3-Haiku")
# A√±adir un separador o algo de contexto
st.markdown("""
Esta es una calculadora financiera inteligente. Puedes pedirle c√°lculos como:
- Valor Actual Neto (VAN)
- Costo Promedio Ponderado de Capital (WACC)
- Valoraci√≥n de Bonos
- Costo de Equity (CAPM)
- Ratio de Sharpe
- Valoraci√≥n de Acciones (Gordon Growth)
- Precio de Opciones Call (Black-Scholes)

**Nota:** Por ahora, por favor haz una solicitud de c√°lculo a la vez.
""")
st.divider() # Separador visual

# --- L√≥gica del Chat ---

# Inicializar historial de chat en st.session_state si no existe
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "¬°Hola! ¬øQu√© c√°lculo financiero necesitas realizar hoy?"}]

# Mostrar mensajes del historial
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Capturar input del usuario
if prompt := st.chat_input("Ej: Calcular VAN para inversi√≥n 50k, flujos [15k, 20k, 25k], tasa 12%"):
    
    # A√±adir mensaje del usuario al historial y mostrarlo
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Preparar entrada para LangGraph
    graph_input = {"messages": [HumanMessage(content=prompt)]}
    # Usar un ID de sesi√≥n consistente para la memoria entre turnos
    config = {"configurable": {"thread_id": "streamlit-user-session"}} 

    # Ejecutar el grafo y mostrar respuesta del asistente
    with st.chat_message("assistant"):
        message_placeholder = st.empty() 
        # Mostrar feedback mientras procesa
        with st.spinner("Calculando... üß†"):
            final_response_content = "" # Variable para el texto final

            try:
                # Usar invoke() para obtener solo el estado final
                final_state = compiled_graph.invoke(graph_input, config=config)
                
                # Procesar el estado final para extraer la respuesta m√°s relevante
                # Iterar desde el final para encontrar la √∫ltima respuesta √∫til del asistente (AIMessage)
                if final_state and "messages" in final_state and final_state["messages"]:
                    for msg in reversed(final_state["messages"]):
                         # Buscamos AIMessage que no sea impl√≠citamente una llamada a herramienta pendiente
                         # (getattr es m√°s seguro que acceso directo)
                         is_final_ai_msg = isinstance(msg, AIMessage) and not getattr(msg, 'tool_calls', []) 
                         if is_final_ai_msg:
                             content = msg.content
                             if isinstance(content, str): 
                                  final_response_content = content
                             elif isinstance(content, list): # Manejar contenido en lista
                                  text_parts = []
                                  for part in content:
                                      if isinstance(part, dict) and 'text' in part:
                                          text_parts.append(part['text'])
                                      elif isinstance(part, str):
                                           text_parts.append(part)
                                  final_response_content = "\n".join(text_parts).strip()
                             
                             # Si encontramos una respuesta con contenido, la usamos y salimos
                             if final_response_content: 
                                  break 
                
                # Fallback si no se encontr√≥ respuesta √∫til
                if not final_response_content:
                    final_response_content = "Lo siento, no pude procesar tu solicitud completamente. ¬øPodr√≠as intentarlo de nuevo o reformular?"
                    print("‚ö†Ô∏è No se encontr√≥ AIMessage final √∫til en el estado final.") # Log para depuraci√≥n

            except Exception as e:
                # Capturar errores durante la ejecuci√≥n del grafo
                final_response_content = f"Ocurri√≥ un error inesperado al procesar tu solicitud."
                import traceback
                error_details = traceback.format_exc()
                print(f"‚ùå ERROR STREAMLIT RUNTIME: {error_details}") 
                st.error(f"{final_response_content} Por favor, intenta de nuevo m√°s tarde.") 
                # No mostramos detalles t√©cnicos al usuario final por defecto
                # message_placeholder.error(f"{final_response_content} Detalles: {e}")
                # Salir del 'with st.spinner' si hubo error
                # st.stop() # Esto detendr√≠a la app, quiz√°s no sea ideal. 
                           # Mejor solo mostrar el error y permitir continuar.

        # Mostrar la respuesta final fuera del spinner
        # Simular efecto de "escritura" para mejor UX
        if final_response_content:
             message_placeholder.markdown(final_response_content)
        else:
             # Si algo muy raro pas√≥ y no hay contenido, mostrar error gen√©rico
             message_placeholder.error("No se pudo obtener una respuesta.")


    # A√±adir respuesta final (o error) al historial de Streamlit
    if final_response_content: # Solo a√±adir si hubo una respuesta (incluso si fue un error manejado)
        st.session_state.messages.append({"role": "assistant", "content": final_response_content})
        # Opcional: st.rerun() para actualizar la interfaz si fuera necesario, 
        # pero para chat usualmente no hace falta si se actualiza session_state.