#!/usr/bin/env python3
"""
generate_index_traditional_enhanced.py
Script de ADMINISTRADOR para indexar libros CFA usando TRADITIONAL CHUNKING MEJORADO.

üéØ OBJETIVO: $0 costos + R√ÅPIDO + Preservar f√≥rmulas financieras
‚ö° VENTAJAS sobre Semantic Chunking:
   1. ‚úÖ Costo: $0 (no usa embeddings durante chunking)
   2. ‚úÖ Velocidad: 2-5 minutos (vs 30-60 minutos con semantic local)
   3. ‚úÖ Preserva f√≥rmulas con separadores financieros inteligentes
   4. ‚úÖ Usa OpenAI embeddings SOLO en indexaci√≥n final (mucho menos llamadas)

üî¨ MEJORAS vs generate_index.py tradicional:
   1. ‚úÖ Separadores financieros espec√≠ficos (ecuaciones, f√≥rmulas, tablas)
   2. ‚úÖ Chunk size optimizado para finanzas (1500 vs 1200)
   3. ‚úÖ Overlap aumentado para preservar contexto (300 vs 250)
   4. ‚úÖ Detecci√≥n de bloques matem√°ticos (no corta en medio de f√≥rmulas)

USO:
1. Coloca tus libros CFA en: ./data/cfa_books/
2. Configura OPENAI_API_KEY en .env
3. Ejecuta: python admin/generate_index_traditional_enhanced.py
4. Los documentos se indexan en Elasticsearch

COSTO ESTIMADO:
- Chunking: $0 (sin embeddings)
- Indexaci√≥n final: ~$0.50-1 (solo embeddings de chunks finales, NO de cada oraci√≥n)
- AHORRO: $49-99 vs semantic chunking con OpenAI

SOLO el administrador ejecuta este script.
"""

import sys
from pathlib import Path
from datetime import datetime
import re

# A√±adir el directorio padre al path para imports
sys.path.insert(0, str(Path(__file__).parent.parent))

# Importar configuraci√≥n de Elasticsearch
from config_elasticsearch import (
    get_elasticsearch_client,
    ES_INDEX_NAME,
    EMBEDDING_MODEL,
    EMBEDDING_DIMENSIONS
)

# Importar API key de OpenAI
from config import OPENAI_API_KEY

# ========================================
# CONFIGURACI√ìN OPTIMIZADA PARA FINANZAS
# ========================================

BOOKS_DIR = Path("./data/cfa_books")

# √çndice mejorado
ENHANCED_INDEX_NAME = ES_INDEX_NAME + "_enhanced"

# Chunking optimizado para material financiero/t√©cnico
ENHANCED_CHUNK_SIZE = 1500  # Aumentado de 1200 para capturar f√≥rmulas completas
ENHANCED_CHUNK_OVERLAP = 300  # Aumentado de 250 para mejor contexto

# Separadores financieros (orden de prioridad)
FINANCIAL_SEPARATORS = [
    # 1. Secciones principales
    "\n\n## ",
    "\n\n### ",
    "\n\n#### ",

    # 2. Bloques de ecuaciones (LaTeX, Markdown)
    "\n$$",  # Ecuaci√≥n LaTeX block
    "\n\\begin{equation}",
    "\n\\begin{align}",

    # 3. Saltos de p√°rrafo
    "\n\n",

    # 4. Puntos de corte l√≥gicos en finanzas
    "\nExample:",
    "\nFormula:",
    "\nEquation:",
    "\nDefinition:",
    "\nTheorem:",
    "\nLearning Outcome:",

    # 5. Saltos de l√≠nea y puntos
    "\n",
    ". ",

    # 6. √öltimo recurso
    " ",
    ""
]

# ========================================
# FUNCIONES
# ========================================

def print_header(text):
    """Imprime un header bonito."""
    print("\n" + "="*60)
    print(f"  {text}")
    print("="*60 + "\n")


def check_prerequisites():
    """Verifica que todo est√© listo."""
    print_header("Verificando Prerrequisitos")

    # 0. Verificar OpenAI API Key
    if not OPENAI_API_KEY:
        print("‚ùå ERROR: OPENAI_API_KEY no encontrada")
        print("   Config√∫rala en .env o como variable de entorno:")
        print("   OPENAI_API_KEY=sk-...")
        sys.exit(1)
    else:
        print(f"‚úÖ OpenAI API Key configurada")
        print(f"   Modelo: {EMBEDDING_MODEL}")
        print(f"   Dimensiones: {EMBEDDING_DIMENSIONS}")
        print(f"   ‚ö° Uso: SOLO para indexaci√≥n final (NO para chunking)")

    # 1. Verificar carpeta de libros
    if not BOOKS_DIR.exists():
        print(f"‚ùå ERROR: No existe la carpeta: {BOOKS_DIR}")
        sys.exit(1)

    # 2. Contar archivos
    pdf_count = len(list(BOOKS_DIR.rglob("*.pdf")))
    txt_count = len(list(BOOKS_DIR.rglob("*.txt")))
    md_count = len(list(BOOKS_DIR.rglob("*.md")))
    total = pdf_count + txt_count + md_count

    print(f"üìö Libros encontrados:")
    print(f"   PDFs: {pdf_count}")
    print(f"   TXTs: {txt_count}")
    print(f"   Markdowns: {md_count}")
    print(f"   TOTAL: {total}")

    if total == 0:
        print(f"\n‚ùå ERROR: No hay archivos en {BOOKS_DIR}")
        sys.exit(1)

    # 3. Verificar dependencias
    try:
        from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader, TextLoader
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        from langchain_openai import OpenAIEmbeddings
        from langchain_elasticsearch import ElasticsearchStore
        from elasticsearch import Elasticsearch
        print("‚úÖ Dependencias instaladas correctamente")
    except ImportError as e:
        print(f"‚ùå ERROR: Falta instalar dependencias")
        print(f"   {e}")
        sys.exit(1)

    # 4. Verificar conexi√≥n a Elasticsearch
    client = get_elasticsearch_client()
    if not client:
        print("‚ùå ERROR: No se pudo conectar a Elasticsearch")
        sys.exit(1)

    print("\n‚úÖ Todos los prerrequisitos cumplidos\n")
    return True


def load_documents():
    """Carga todos los documentos."""
    print_header("Cargando Documentos")

    from langchain_community.document_loaders import (
        DirectoryLoader,
        TextLoader,
        PyPDFLoader,
    )

    all_docs = []

    # PDFs
    print("üìÑ Cargando PDFs...")
    try:
        pdf_loader = DirectoryLoader(
            str(BOOKS_DIR),
            glob="**/*.pdf",
            loader_cls=PyPDFLoader,
            show_progress=True
        )
        pdf_docs = pdf_loader.load()
        all_docs.extend(pdf_docs)
        print(f"‚úÖ {len(pdf_docs)} PDFs cargados\n")
    except Exception as e:
        print(f"‚ö†Ô∏è  Error cargando PDFs: {e}\n")

    # TXTs
    print("üìù Cargando archivos TXT...")
    try:
        txt_loader = DirectoryLoader(
            str(BOOKS_DIR),
            glob="**/*.txt",
            loader_cls=TextLoader,
            show_progress=True
        )
        txt_docs = txt_loader.load()
        all_docs.extend(txt_docs)
        print(f"‚úÖ {len(txt_docs)} TXTs cargados\n")
    except Exception as e:
        print(f"‚ö†Ô∏è  Error cargando TXTs: {e}\n")

    print(f"üìö TOTAL DOCUMENTOS CARGADOS: {len(all_docs)}\n")
    return all_docs


def detect_formula_blocks(text: str) -> bool:
    """
    Detecta si un texto contiene bloques de f√≥rmulas matem√°ticas.
    Esto ayuda a evitar cortes en medio de ecuaciones.
    """
    # Patrones de f√≥rmulas financieras
    formula_patterns = [
        r'\$\$.*?\$\$',  # LaTeX display mode
        r'\\begin\{equation\}',
        r'\\begin\{align\}',
        r'\\frac\{',
        r'\\sum',
        r'\\int',
        r'NPV\s*=',
        r'IRR\s*=',
        r'WACC\s*=',
        r'Beta\s*=',
        r'E\(R\)\s*=',  # Expected Return
    ]

    for pattern in formula_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False


def split_documents_enhanced(documents):
    """
    Divide documentos usando TRADITIONAL CHUNKING MEJORADO para finanzas.

    üéØ MEJORAS:
    1. ‚úÖ Separadores financieros espec√≠ficos
    2. ‚úÖ Chunk size optimizado (1500 vs 1200)
    3. ‚úÖ Overlap aumentado (300 vs 250)
    4. ‚úÖ Sin costo de embeddings (vs $50-100 con semantic)

    Args:
        documents: Lista de documentos de LangChain

    Returns:
        Lista de chunks optimizados
    """
    print_header("Fragmentaci√≥n MEJORADA para Material Financiero")

    from langchain.text_splitter import RecursiveCharacterTextSplitter

    print(f"‚úÇÔ∏è  Configuraci√≥n MEJORADA:")
    print(f"   Chunk size: {ENHANCED_CHUNK_SIZE} (vs 1200 tradicional)")
    print(f"   Overlap: {ENHANCED_CHUNK_OVERLAP} (vs 250 tradicional)")
    print(f"   Separadores: {len(FINANCIAL_SEPARATORS)} espec√≠ficos para finanzas")
    print(f"   üí∞ Costo chunking: $0 (sin embeddings)")
    print(f"\nüìã Separadores financieros usados:")
    print(f"   1. Secciones (##, ###)")
    print(f"   2. Bloques de ecuaciones ($$, \\begin{{equation}})")
    print(f"   3. Puntos l√≥gicos (Example:, Formula:, Definition:)")
    print(f"   4. Saltos de p√°rrafo y puntos\n")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=ENHANCED_CHUNK_SIZE,
        chunk_overlap=ENHANCED_CHUNK_OVERLAP,
        length_function=len,
        separators=FINANCIAL_SEPARATORS
    )

    chunks = text_splitter.split_documents(documents)

    # A√±adir metadata adicional
    formula_chunks = 0
    for i, chunk in enumerate(chunks):
        source = chunk.metadata.get('source', '')

        # Detectar Level CFA
        if 'Level_I' in source or 'Level_1' in source:
            chunk.metadata['cfa_level'] = 'I'
        elif 'Level_II' in source or 'Level_2' in source:
            chunk.metadata['cfa_level'] = 'II'
        elif 'Level_III' in source or 'Level_3' in source:
            chunk.metadata['cfa_level'] = 'III'

        chunk.metadata['chunk_id'] = f"chunk_{i+1}"
        chunk.metadata['indexed_at'] = datetime.now().isoformat()

        # Detectar si contiene f√≥rmulas
        if detect_formula_blocks(chunk.page_content):
            chunk.metadata['contains_formulas'] = True
            formula_chunks += 1
        else:
            chunk.metadata['contains_formulas'] = False

    print(f"‚úÖ {len(chunks)} chunks creados")
    print(f"   Promedio: {len(chunks) / max(len(documents), 1):.1f} chunks por documento")
    print(f"   üìê Chunks con f√≥rmulas: {formula_chunks} ({formula_chunks / len(chunks) * 100:.1f}%)\n")

    # Estad√≠sticas
    chunk_sizes = [len(chunk.page_content) for chunk in chunks]
    avg_size = sum(chunk_sizes) / len(chunk_sizes) if chunk_sizes else 0
    min_size = min(chunk_sizes) if chunk_sizes else 0
    max_size = max(chunk_sizes) if chunk_sizes else 0

    print(f"üìè Estad√≠sticas de tama√±o:")
    print(f"   Promedio: {avg_size:.0f} caracteres")
    print(f"   M√≠nimo: {min_size} caracteres")
    print(f"   M√°ximo: {max_size} caracteres\n")

    return chunks


def create_or_recreate_index(es_client):
    """Crea o recrea el √≠ndice mejorado en Elasticsearch."""
    print_header("Configurando √çndice en Elasticsearch")

    if es_client.indices.exists(index=ENHANCED_INDEX_NAME):
        print(f"‚ö†Ô∏è  El √≠ndice '{ENHANCED_INDEX_NAME}' ya existe.")
        response = input("¬øDeseas eliminarlo y recrearlo? (s/n): ")

        if response.lower() == 's':
            print(f"üóëÔ∏è  Eliminando √≠ndice '{ENHANCED_INDEX_NAME}'...")
            es_client.indices.delete(index=ENHANCED_INDEX_NAME)
            print("‚úÖ √çndice eliminado")
        else:
            print("‚ÑπÔ∏è  Los documentos se a√±adir√°n al √≠ndice existente")
            return

    print(f"üî® Creando √≠ndice '{ENHANCED_INDEX_NAME}'...")

    index_mapping = {
        "mappings": {
            "properties": {
                "text": {"type": "text"},
                "vector": {
                    "type": "dense_vector",
                    "dims": EMBEDDING_DIMENSIONS,  # 1536 para OpenAI
                    "index": True,
                    "similarity": "cosine"
                },
                "metadata": {"type": "object"}
            }
        }
    }

    es_client.indices.create(index=ENHANCED_INDEX_NAME, body=index_mapping)
    print(f"‚úÖ √çndice '{ENHANCED_INDEX_NAME}' creado\n")


def estimate_tokens(text: str) -> int:
    """Estima la cantidad de tokens en un texto."""
    return len(text) // 4


def create_batches(chunks, max_tokens_per_batch=250000):
    """Divide chunks en batches que no excedan el l√≠mite de tokens."""
    batches = []
    current_batch = []
    current_tokens = 0

    for chunk in chunks:
        chunk_tokens = estimate_tokens(chunk.page_content)

        if current_tokens + chunk_tokens > max_tokens_per_batch and current_batch:
            batches.append(current_batch)
            current_batch = [chunk]
            current_tokens = chunk_tokens
        else:
            current_batch.append(chunk)
            current_tokens += chunk_tokens

    if current_batch:
        batches.append(current_batch)

    return batches


def index_documents_to_elasticsearch(chunks):
    """Indexa los chunks en Elasticsearch usando OpenAI Embeddings."""
    print_header("Indexando Documentos en Elasticsearch")

    from langchain_openai import OpenAIEmbeddings
    from langchain_elasticsearch import ElasticsearchStore
    from config_elasticsearch import get_es_config

    print(f"üß† Modelo de embeddings OpenAI: {EMBEDDING_MODEL}")
    print(f"   Dimensiones: {EMBEDDING_DIMENSIONS}")
    print(f"   üí∞ Costo: ~$0.50-1 (SOLO para {len(chunks)} chunks finales)")
    print(f"   ‚ö° Velocidad: ~1-2 minutos\n")

    if not OPENAI_API_KEY:
        print("‚ùå ERROR: OPENAI_API_KEY no encontrada")
        sys.exit(1)

    embeddings = OpenAIEmbeddings(
        model=EMBEDDING_MODEL,
        openai_api_key=OPENAI_API_KEY,
        chunk_size=500,
        max_retries=3
    )

    es_config = get_es_config()

    # Crear batches
    print(f"üì¶ Creando batches de documentos...")
    batches = create_batches(chunks, max_tokens_per_batch=250000)
    print(f"   Total chunks: {len(chunks)}")
    print(f"   Total batches: {len(batches)}")
    print(f"   Chunks por batch (aprox): {len(chunks) // len(batches) if batches else 0}\n")

    try:
        vector_store = None
        total_indexed = 0

        for i, batch in enumerate(batches, 1):
            print(f"üì§ Procesando batch {i}/{len(batches)} ({len(batch)} chunks)...")

            if i == 1:
                vector_store = ElasticsearchStore.from_documents(
                    documents=batch,
                    embedding=embeddings,
                    index_name=ENHANCED_INDEX_NAME,
                    es_url=es_config["es_url"],
                    es_user=es_config["es_user"],
                    es_password=es_config["es_password"],
                    bulk_kwargs={"request_timeout": 120}
                )
            else:
                vector_store.add_documents(
                    documents=batch,
                    bulk_kwargs={"request_timeout": 120}
                )

            total_indexed += len(batch)
            print(f"   ‚úÖ Batch {i} completado ({total_indexed}/{len(chunks)} chunks indexados)")

        print(f"\n‚úÖ Todos los documentos indexados exitosamente ({total_indexed} chunks)\n")
        return True

    except Exception as e:
        print(f"‚ùå ERROR indexando documentos: {e}")
        import traceback
        traceback.print_exc()
        return False


def verify_index():
    """Verifica que el √≠ndice se haya creado correctamente."""
    print_header("Verificando √çndice")

    es_client = get_elasticsearch_client()

    try:
        count = es_client.count(index=ENHANCED_INDEX_NAME)
        doc_count = count['count']

        print(f"‚úÖ √çndice verificado:")
        print(f"   Nombre: {ENHANCED_INDEX_NAME}")
        print(f"   Documentos: {doc_count}")

        sample = es_client.search(index=ENHANCED_INDEX_NAME, size=1)
        if sample['hits']['hits']:
            print(f"   Estado: Activo y funcional ‚úÖ\n")

        return True

    except Exception as e:
        print(f"‚ùå Error verificando √≠ndice: {e}")
        return False


def main():
    """Funci√≥n principal."""
    print("\n" + "üöÄ"*30)
    print("  INDEXADOR MEJORADO - Sistema CFA")
    print("  Traditional Chunking + Separadores Financieros")
    print("  üí∞ COSTO: ~$0.50-1 (vs $50-100 semantic)")
    print("  ‚ö° VELOCIDAD: 2-5 minutos (vs 30-60 semantic local)")
    print("üöÄ"*30)

    print(f"\nüìÖ Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üìÇ Libros: {BOOKS_DIR}")
    print(f"üì¶ √çndice ES: {ENHANCED_INDEX_NAME}")
    print(f"üß† Embeddings: {EMBEDDING_MODEL} (OpenAI, SOLO indexaci√≥n final)")
    print(f"‚úÇÔ∏è  Chunking: Mejorado (sin embeddings)")
    print(f"üí∞ Costo estimado: ~$0.50-1")
    print(f"‚è±Ô∏è  Tiempo estimado: 2-5 minutos\n")

    response = input("¬øDeseas continuar? (s/n): ")
    if response.lower() != 's':
        print("‚ùå Cancelado por el usuario.")
        sys.exit(0)

    try:
        # 1. Verificar prerrequisitos
        check_prerequisites()

        # 2. Obtener cliente ES
        es_client = get_elasticsearch_client()
        if not es_client:
            print("‚ùå No se pudo conectar a Elasticsearch")
            sys.exit(1)

        # 3. Configurar √≠ndice
        create_or_recreate_index(es_client)

        # 4. Cargar documentos
        documents = load_documents()

        if not documents:
            print("‚ùå ERROR: No se cargaron documentos.")
            sys.exit(1)

        # 5. Dividir en chunks MEJORADOS
        chunks = split_documents_enhanced(documents)

        # 6. Indexar en Elasticsearch
        success = index_documents_to_elasticsearch(chunks)

        if not success:
            print("‚ùå ERROR: Fallo en la indexaci√≥n")
            sys.exit(1)

        # 7. Verificar
        verify_index()

        # Resumen final
        print_header("‚úÖ PROCESO COMPLETADO EXITOSAMENTE")
        print(f"üìä Resumen:")
        print(f"   - Documentos procesados: {len(documents)}")
        print(f"   - Chunks generados: {len(chunks)}")
        print(f"   - √çndice Elasticsearch: {ENHANCED_INDEX_NAME}")
        print(f"   - Embeddings: OpenAI {EMBEDDING_MODEL}")
        print(f"   - Chunking: Mejorado con separadores financieros")
        print(f"   - Costo: ~$0.50-1 (98% ahorro vs semantic OpenAI)")
        print(f"   - Tiempo: 2-5 minutos (95% m√°s r√°pido vs semantic local)")
        print(f"\nüéØ Ventajas:")
        print(f"   ‚úÖ F√≥rmulas financieras preservadas con separadores inteligentes")
        print(f"   ‚úÖ R√°pido y econ√≥mico")
        print(f"   ‚úÖ Dimensiones compatibles con OpenAI (1536)\n")

    except KeyboardInterrupt:
        print("\n\n‚ùå Proceso cancelado por el usuario.")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå ERROR CR√çTICO: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
